{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4d61e-bc57-42bd-9cda-14b8a96139ca",
   "metadata": {},
   "source": [
    "最初に'output_video.mp4' という名前の動画ファイルをフォルダに入れてください\n",
    "\n",
    "参照：https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/vision-background-removal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a188eae-3523-4d0f-b51f-0e368b42b99f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820b1562-0b9f-469d-b2d6-b1ee92307472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision pillow huggingface_hub \"openvino>=2024.0.0\" matplotlib \"gradio>=4.15\" \"transformers>=4.39.1\" tqdm --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a01208-4961-4676-8eda-8947b795d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01078eeb-220b-42ba-9b12-add47b8dc93d",
   "metadata": {},
   "source": [
    "Download model code from HuggingFace hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f236ad39-c0a3-43be-8cd5-4d7342abd7db",
   "metadata": {},
   "source": [
    "## Load PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For loading model using PyTorch, we should use `AutoModelForImageSegmentation.from_pretrained` method. Model weights will be downloaded automatically during first model usage. Please, note, it may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29155943-645a-416d-893a-a9d203380990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageSegmentation\n",
    "\n",
    "net = AutoModelForImageSegmentation.from_pretrained(\"briaai/RMBG-1.4\", trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6458ec72-3a06-49a5-ab9e-51f0f8344c01",
   "metadata": {},
   "source": [
    "## Convert Model to OpenVINO Intermediate Representation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdaae7a-ff4a-4a4b-8d76-a1520806f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import normalize\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(im: np.ndarray, model_input_size: list) -> torch.Tensor:\n",
    "    if len(im.shape) < 3:\n",
    "        im = im[:, :, np.newaxis]\n",
    "    # orig_im_size=im.shape[0:2]\n",
    "    im_tensor = torch.tensor(im, dtype=torch.float32).permute(2,0,1)\n",
    "    im_tensor = F.interpolate(torch.unsqueeze(im_tensor,0), size=model_input_size, mode='bilinear').type(torch.uint8)\n",
    "    image = torch.divide(im_tensor,255.0)\n",
    "    image = normalize(image,[0.5,0.5,0.5],[1.0,1.0,1.0])\n",
    "    return image\n",
    "\n",
    "\n",
    "def postprocess_image(result: torch.Tensor, im_size: list)-> np.ndarray:\n",
    "    result = torch.squeeze(F.interpolate(result, size=im_size, mode='bilinear') ,0)\n",
    "    ma = torch.max(result)\n",
    "    mi = torch.min(result)\n",
    "    result = (result-mi)/(ma-mi)\n",
    "    im_array = (result*255).permute(1,2,0).cpu().data.numpy().astype(np.uint8)\n",
    "    im_array = np.squeeze(im_array)\n",
    "    return im_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9515d4c4-81eb-4a67-b911-3ceb0f1480e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageSegmentation\n",
    "import cv2\n",
    "\n",
    "net = AutoModelForImageSegmentation.from_pretrained(\"briaai/RMBG-1.4\", trust_remote_code=True)\n",
    "\n",
    "ov_model_path = Path(\"rmbg-1.4.xml\")\n",
    "model_input_size = [1024, 1024]\n",
    "\n",
    "cap = cv2.VideoCapture(\"output_video.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"動画ファイルを開けませんでした。\")\n",
    "    exit()\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "image = preprocess_image(frame, model_input_size)\n",
    "\n",
    "if not ov_model_path.exists():\n",
    "    ov_model = ov.convert_model(net, example_input=image, input=[1, 3, *model_input_size])\n",
    "    ov.save_model(ov_model, ov_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97e0b11a-bfcc-4ef9-9c30-3cf2e6147911",
   "metadata": {},
   "source": [
    "## Run OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "After finishing conversion, we can compile converted model and run it using OpenVINO on specified device.\n",
    "For selection inference device, please use dropdown list below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da00a150-bd94-4ed0-8591-4340bba75682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d458344359643d59a15ecc26ed0ad4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'NPU', 'AUTO'), value='NPU')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"NPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "724cb7e0-e6c2-412f-98f8-7c89cadf2092",
   "metadata": {},
   "source": [
    "Let's run model on the same image that we used before for launching PyTorch model. OpenVINO model input and output is fully compatible with original pre- and postprocessing steps, it means that we can reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c16d89-1d62-4e37-ab9f-7b76982d4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import AutoModelForImageSegmentation\n",
    " \n",
    "ov_compiled_model = core.compile_model(ov_model_path, device.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15ec062-4759-47df-9ea7-949de6a1aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "cap = cv2.VideoCapture(r'output_video.mp4')\n",
    "\n",
    "if (cap.isOpened()== False):  \n",
    "  print(\"ビデオファイルを開くとエラーが発生しました\") \n",
    "\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "model_input_size = [1024, 1024]\n",
    "\n",
    "orig_im_size = [height, width]\n",
    "\n",
    "no_bg_image = Image.new(\"RGBA\", orig_im_size, (0, 0, 0, 0))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        image = preprocess_image(np.array(frame), model_input_size)\n",
    "        \n",
    "        #result = ov_compiled_model(torch.from_numpy(image))[0]\n",
    "        result = ov_compiled_model(image)[0]\n",
    "\n",
    "        # post process\n",
    "        result_image = postprocess_image(torch.from_numpy(result), orig_im_size)\n",
    "        pil_im = Image.fromarray(result_image)\n",
    "\n",
    "        ori_image = Image.fromarray(frame)\n",
    "        no_bg_image = Image.new(\"RGBA\", [width, height], (0, 0, 0, 0))\n",
    "        no_bg_image.paste(ori_image, mask=pil_im)\n",
    "\n",
    "        # Play\n",
    "    \n",
    "        cv2.imshow(\"Video\", np.array(no_bg_image))\n",
    "        \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a5cca-b24f-47e3-b2c2-908be1a7db73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/a2fdaeec-b7a3-45f5-b307-ca89d447094d",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Segmentation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
